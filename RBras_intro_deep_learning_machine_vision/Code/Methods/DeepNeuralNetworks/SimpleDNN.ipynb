{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7051e1e0",
   "metadata": {},
   "source": [
    "# Deep Neural Networks application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a63151",
   "metadata": {},
   "source": [
    "Purpose: Train a DNN based on features extracted from insect images (Ong, 2022)\n",
    "\n",
    "Authors: Gabriel R. Palma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd0549",
   "metadata": {},
   "source": [
    "## Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374c007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Importing section\n",
    "from tensorflow.keras import models\n",
    "from keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "# Paper Correction\n",
    "from keras import metrics\n",
    "# End importing section\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from scipy.io import wavfile\n",
    "import pylab\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "datagen = ImageDataGenerator(rescale=1)\n",
    "batch_size = 32\n",
    "from keras.layers import LeakyReLU, Conv2D, Input, BatchNormalization, Activation, Dense, Dropout, Conv2DTranspose, concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a1347",
   "metadata": {},
   "source": [
    "# Functions used in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513dc6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(explanatory_variables):\n",
    "    '''This function scale the variables based on their maximum and minimun value'''\n",
    "    \n",
    "    scaler = MinMaxScaler() # Scaling the variables\n",
    "    scaler.fit(explanatory_variables)\n",
    "    explanatory_variables = scaler.transform(explanatory_variables)\n",
    "    \n",
    "    return(explanatory_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e129327",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f0f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_dnn = pd.read_csv('../../output_data/Diptera_parameters_train.csv').drop(columns = 'Unnamed: 0')\n",
    "test_dataset_dnn = pd.read_csv('../../output_data/Diptera_parameters_test.csv').drop(columns = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3f59a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aspect ratio</th>\n",
       "      <th>Countour area</th>\n",
       "      <th>R average</th>\n",
       "      <th>G average</th>\n",
       "      <th>B average</th>\n",
       "      <th>R 0.25 % quantile</th>\n",
       "      <th>G 0.25 % quantile</th>\n",
       "      <th>B 0.25 % quantile</th>\n",
       "      <th>R 97.5 % quantile</th>\n",
       "      <th>G 97.5 % quantile</th>\n",
       "      <th>B 97.5 % quantile</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.948529</td>\n",
       "      <td>5748.0</td>\n",
       "      <td>220.702009</td>\n",
       "      <td>219.789820</td>\n",
       "      <td>221.338010</td>\n",
       "      <td>51.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.168224</td>\n",
       "      <td>7045.5</td>\n",
       "      <td>208.658363</td>\n",
       "      <td>206.978755</td>\n",
       "      <td>208.423489</td>\n",
       "      <td>52.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.969925</td>\n",
       "      <td>6747.5</td>\n",
       "      <td>231.006736</td>\n",
       "      <td>230.377890</td>\n",
       "      <td>230.878388</td>\n",
       "      <td>60.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.863946</td>\n",
       "      <td>6956.5</td>\n",
       "      <td>213.174027</td>\n",
       "      <td>211.627870</td>\n",
       "      <td>212.719946</td>\n",
       "      <td>43.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.269565</td>\n",
       "      <td>7164.5</td>\n",
       "      <td>198.287428</td>\n",
       "      <td>194.817323</td>\n",
       "      <td>197.512357</td>\n",
       "      <td>48.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>0.734513</td>\n",
       "      <td>4162.0</td>\n",
       "      <td>196.237643</td>\n",
       "      <td>194.549705</td>\n",
       "      <td>195.608119</td>\n",
       "      <td>59.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>0.993377</td>\n",
       "      <td>8976.0</td>\n",
       "      <td>183.377332</td>\n",
       "      <td>182.305385</td>\n",
       "      <td>183.374402</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>0.734807</td>\n",
       "      <td>10114.5</td>\n",
       "      <td>208.519272</td>\n",
       "      <td>206.502790</td>\n",
       "      <td>207.808873</td>\n",
       "      <td>51.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>6993.0</td>\n",
       "      <td>222.760583</td>\n",
       "      <td>220.487524</td>\n",
       "      <td>222.041992</td>\n",
       "      <td>67.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>0.826087</td>\n",
       "      <td>7074.0</td>\n",
       "      <td>213.233598</td>\n",
       "      <td>211.385403</td>\n",
       "      <td>212.894890</td>\n",
       "      <td>64.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>848 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Aspect ratio  Countour area   R average   G average   B average  \\\n",
       "0        0.948529         5748.0  220.702009  219.789820  221.338010   \n",
       "1        1.168224         7045.5  208.658363  206.978755  208.423489   \n",
       "2        0.969925         6747.5  231.006736  230.377890  230.878388   \n",
       "3        0.863946         6956.5  213.174027  211.627870  212.719946   \n",
       "4        1.269565         7164.5  198.287428  194.817323  197.512357   \n",
       "..            ...            ...         ...         ...         ...   \n",
       "843      0.734513         4162.0  196.237643  194.549705  195.608119   \n",
       "844      0.993377         8976.0  183.377332  182.305385  183.374402   \n",
       "845      0.734807        10114.5  208.519272  206.502790  207.808873   \n",
       "846      0.750000         6993.0  222.760583  220.487524  222.041992   \n",
       "847      0.826087         7074.0  213.233598  211.385403  212.894890   \n",
       "\n",
       "     R 0.25 % quantile  G 0.25 % quantile  B 0.25 % quantile  \\\n",
       "0                 51.0               56.0               62.0   \n",
       "1                 52.0               44.0               51.0   \n",
       "2                 60.0               61.0               62.0   \n",
       "3                 43.0               37.0               40.0   \n",
       "4                 48.0               36.0               47.0   \n",
       "..                 ...                ...                ...   \n",
       "843               59.0               35.0               46.0   \n",
       "844               21.0               20.0               21.0   \n",
       "845               51.0               43.0               48.0   \n",
       "846               67.0               58.0               64.0   \n",
       "847               64.0               51.0               61.0   \n",
       "\n",
       "     R 97.5 % quantile  G 97.5 % quantile  B 97.5 % quantile  Class  \n",
       "0                238.0              238.0              239.0    1.0  \n",
       "1                226.0              226.0              227.0    3.0  \n",
       "2                249.0              249.0              249.0    1.0  \n",
       "3                233.0              233.0              233.0    0.0  \n",
       "4                215.0              215.0              216.0    2.0  \n",
       "..                 ...                ...                ...    ...  \n",
       "843              208.0              208.0              208.0    4.0  \n",
       "844              209.0              209.0              209.0    0.0  \n",
       "845              229.0              229.0              229.0    3.0  \n",
       "846              242.0              242.0              242.0    3.0  \n",
       "847              229.0              229.0              229.0    3.0  \n",
       "\n",
       "[848 rows x 12 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fad7efec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">Aspect ratio</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Countour area</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">G 97.5 % quantile</th>\n",
       "      <th colspan=\"8\" halign=\"left\">B 97.5 % quantile</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>530.0</td>\n",
       "      <td>0.964924</td>\n",
       "      <td>0.229992</td>\n",
       "      <td>0.358025</td>\n",
       "      <td>0.797834</td>\n",
       "      <td>0.959730</td>\n",
       "      <td>1.086420</td>\n",
       "      <td>1.615385</td>\n",
       "      <td>530.0</td>\n",
       "      <td>3113.092453</td>\n",
       "      <td>...</td>\n",
       "      <td>252.75</td>\n",
       "      <td>255.0</td>\n",
       "      <td>530.0</td>\n",
       "      <td>248.167925</td>\n",
       "      <td>7.276351</td>\n",
       "      <td>221.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>409.0</td>\n",
       "      <td>1.047181</td>\n",
       "      <td>0.242451</td>\n",
       "      <td>0.523179</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>1.026316</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.634409</td>\n",
       "      <td>409.0</td>\n",
       "      <td>6692.558680</td>\n",
       "      <td>...</td>\n",
       "      <td>249.00</td>\n",
       "      <td>254.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>246.466993</td>\n",
       "      <td>5.112893</td>\n",
       "      <td>220.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>341.0</td>\n",
       "      <td>1.009748</td>\n",
       "      <td>0.311083</td>\n",
       "      <td>0.424390</td>\n",
       "      <td>0.786585</td>\n",
       "      <td>0.965278</td>\n",
       "      <td>1.221154</td>\n",
       "      <td>1.886076</td>\n",
       "      <td>341.0</td>\n",
       "      <td>7120.807918</td>\n",
       "      <td>...</td>\n",
       "      <td>243.00</td>\n",
       "      <td>252.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>237.674487</td>\n",
       "      <td>9.434740</td>\n",
       "      <td>211.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>398.0</td>\n",
       "      <td>1.050112</td>\n",
       "      <td>0.330838</td>\n",
       "      <td>0.448087</td>\n",
       "      <td>0.776071</td>\n",
       "      <td>1.031754</td>\n",
       "      <td>1.294240</td>\n",
       "      <td>1.880435</td>\n",
       "      <td>398.0</td>\n",
       "      <td>7692.768844</td>\n",
       "      <td>...</td>\n",
       "      <td>242.00</td>\n",
       "      <td>251.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>237.042714</td>\n",
       "      <td>8.288299</td>\n",
       "      <td>206.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>349.0</td>\n",
       "      <td>0.776385</td>\n",
       "      <td>0.286367</td>\n",
       "      <td>0.311828</td>\n",
       "      <td>0.581522</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.945736</td>\n",
       "      <td>3.343284</td>\n",
       "      <td>349.0</td>\n",
       "      <td>4935.553009</td>\n",
       "      <td>...</td>\n",
       "      <td>223.00</td>\n",
       "      <td>234.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>213.687679</td>\n",
       "      <td>11.319163</td>\n",
       "      <td>183.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>235.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Aspect ratio                                                    \\\n",
       "             count      mean       std       min       25%       50%   \n",
       "Class                                                                  \n",
       "0.0          530.0  0.964924  0.229992  0.358025  0.797834  0.959730   \n",
       "1.0          409.0  1.047181  0.242451  0.523179  0.854545  1.026316   \n",
       "2.0          341.0  1.009748  0.311083  0.424390  0.786585  0.965278   \n",
       "3.0          398.0  1.050112  0.330838  0.448087  0.776071  1.031754   \n",
       "4.0          349.0  0.776385  0.286367  0.311828  0.581522  0.721805   \n",
       "\n",
       "                          Countour area               ... G 97.5 % quantile  \\\n",
       "            75%       max         count         mean  ...               75%   \n",
       "Class                                                 ...                     \n",
       "0.0    1.086420  1.615385         530.0  3113.092453  ...            252.75   \n",
       "1.0    1.250000  1.634409         409.0  6692.558680  ...            249.00   \n",
       "2.0    1.221154  1.886076         341.0  7120.807918  ...            243.00   \n",
       "3.0    1.294240  1.880435         398.0  7692.768844  ...            242.00   \n",
       "4.0    0.945736  3.343284         349.0  4935.553009  ...            223.00   \n",
       "\n",
       "             B 97.5 % quantile                                              \\\n",
       "         max             count        mean        std    min    25%    50%   \n",
       "Class                                                                        \n",
       "0.0    255.0             530.0  248.167925   7.276351  221.0  245.0  250.0   \n",
       "1.0    254.0             409.0  246.466993   5.112893  220.0  244.0  247.0   \n",
       "2.0    252.0             341.0  237.674487   9.434740  211.0  232.0  238.0   \n",
       "3.0    251.0             398.0  237.042714   8.288299  206.0  234.0  237.0   \n",
       "4.0    234.0             349.0  213.687679  11.319163  183.0  206.0  214.0   \n",
       "\n",
       "                     \n",
       "         75%    max  \n",
       "Class                \n",
       "0.0    254.0  255.0  \n",
       "1.0    249.0  255.0  \n",
       "2.0    245.0  255.0  \n",
       "3.0    243.0  255.0  \n",
       "4.0    223.0  235.0  \n",
       "\n",
       "[5 rows x 88 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_dnn.groupby('Class').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bacf1fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">Aspect ratio</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Countour area</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">G 97.5 % quantile</th>\n",
       "      <th colspan=\"8\" halign=\"left\">B 97.5 % quantile</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>201.0</td>\n",
       "      <td>1.053810</td>\n",
       "      <td>0.247217</td>\n",
       "      <td>0.528455</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>1.036697</td>\n",
       "      <td>1.206349</td>\n",
       "      <td>1.781250</td>\n",
       "      <td>201.0</td>\n",
       "      <td>6318.557214</td>\n",
       "      <td>...</td>\n",
       "      <td>252.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>243.024876</td>\n",
       "      <td>14.676320</td>\n",
       "      <td>206.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>255.00</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>177.0</td>\n",
       "      <td>0.927893</td>\n",
       "      <td>0.201034</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.918699</td>\n",
       "      <td>1.078947</td>\n",
       "      <td>1.548780</td>\n",
       "      <td>177.0</td>\n",
       "      <td>6050.457627</td>\n",
       "      <td>...</td>\n",
       "      <td>243.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>238.090395</td>\n",
       "      <td>8.009424</td>\n",
       "      <td>213.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>244.00</td>\n",
       "      <td>249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>147.0</td>\n",
       "      <td>1.062164</td>\n",
       "      <td>0.247814</td>\n",
       "      <td>0.511236</td>\n",
       "      <td>0.844541</td>\n",
       "      <td>1.076471</td>\n",
       "      <td>1.212912</td>\n",
       "      <td>1.719512</td>\n",
       "      <td>147.0</td>\n",
       "      <td>8544.537415</td>\n",
       "      <td>...</td>\n",
       "      <td>243.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>236.949830</td>\n",
       "      <td>11.029931</td>\n",
       "      <td>209.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>247.00</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>172.0</td>\n",
       "      <td>0.904104</td>\n",
       "      <td>0.231282</td>\n",
       "      <td>0.401099</td>\n",
       "      <td>0.723463</td>\n",
       "      <td>0.894338</td>\n",
       "      <td>1.061641</td>\n",
       "      <td>1.589744</td>\n",
       "      <td>172.0</td>\n",
       "      <td>7272.418605</td>\n",
       "      <td>...</td>\n",
       "      <td>235.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>229.517442</td>\n",
       "      <td>9.178206</td>\n",
       "      <td>205.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>235.25</td>\n",
       "      <td>248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>151.0</td>\n",
       "      <td>0.833263</td>\n",
       "      <td>0.242269</td>\n",
       "      <td>0.323077</td>\n",
       "      <td>0.665230</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.987630</td>\n",
       "      <td>1.475000</td>\n",
       "      <td>151.0</td>\n",
       "      <td>4292.384106</td>\n",
       "      <td>...</td>\n",
       "      <td>213.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>207.090232</td>\n",
       "      <td>9.258478</td>\n",
       "      <td>186.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>213.00</td>\n",
       "      <td>228.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Aspect ratio                                                    \\\n",
       "             count      mean       std       min       25%       50%   \n",
       "Class                                                                  \n",
       "0.0          201.0  1.053810  0.247217  0.528455  0.869565  1.036697   \n",
       "1.0          177.0  0.927893  0.201034  0.436364  0.782609  0.918699   \n",
       "2.0          147.0  1.062164  0.247814  0.511236  0.844541  1.076471   \n",
       "3.0          172.0  0.904104  0.231282  0.401099  0.723463  0.894338   \n",
       "4.0          151.0  0.833263  0.242269  0.323077  0.665230  0.803571   \n",
       "\n",
       "                          Countour area               ... G 97.5 % quantile  \\\n",
       "            75%       max         count         mean  ...               75%   \n",
       "Class                                                 ...                     \n",
       "0.0    1.206349  1.781250         201.0  6318.557214  ...             252.0   \n",
       "1.0    1.078947  1.548780         177.0  6050.457627  ...             243.0   \n",
       "2.0    1.212912  1.719512         147.0  8544.537415  ...             243.0   \n",
       "3.0    1.061641  1.589744         172.0  7272.418605  ...             235.0   \n",
       "4.0    0.987630  1.475000         151.0  4292.384106  ...             213.0   \n",
       "\n",
       "             B 97.5 % quantile                                              \\\n",
       "         max             count        mean        std    min    25%    50%   \n",
       "Class                                                                        \n",
       "0.0    254.0             201.0  243.024876  14.676320  206.0  233.0  249.0   \n",
       "1.0    249.0             177.0  238.090395   8.009424  213.0  236.0  239.0   \n",
       "2.0    252.0             147.0  236.949830  11.029931  209.0  231.0  236.0   \n",
       "3.0    248.0             172.0  229.517442   9.178206  205.0  226.0  230.0   \n",
       "4.0    227.0             151.0  207.090232   9.258478  186.0  201.0  207.0   \n",
       "\n",
       "                      \n",
       "          75%    max  \n",
       "Class                 \n",
       "0.0    255.00  255.0  \n",
       "1.0    244.00  249.0  \n",
       "2.0    247.00  255.0  \n",
       "3.0    235.25  248.0  \n",
       "4.0    213.00  228.0  \n",
       "\n",
       "[5 rows x 88 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_dnn.groupby('Class').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05572d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aspect ratio</th>\n",
       "      <th>Countour area</th>\n",
       "      <th>R average</th>\n",
       "      <th>G average</th>\n",
       "      <th>B average</th>\n",
       "      <th>R 0.25 % quantile</th>\n",
       "      <th>G 0.25 % quantile</th>\n",
       "      <th>B 0.25 % quantile</th>\n",
       "      <th>R 97.5 % quantile</th>\n",
       "      <th>G 97.5 % quantile</th>\n",
       "      <th>B 97.5 % quantile</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.409722</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>188.999621</td>\n",
       "      <td>187.302455</td>\n",
       "      <td>188.562101</td>\n",
       "      <td>72.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>3194.5</td>\n",
       "      <td>192.006318</td>\n",
       "      <td>190.058972</td>\n",
       "      <td>191.284239</td>\n",
       "      <td>90.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.726316</td>\n",
       "      <td>2955.5</td>\n",
       "      <td>193.060148</td>\n",
       "      <td>192.110591</td>\n",
       "      <td>192.664501</td>\n",
       "      <td>110.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.473214</td>\n",
       "      <td>6355.0</td>\n",
       "      <td>202.093351</td>\n",
       "      <td>199.387297</td>\n",
       "      <td>201.049964</td>\n",
       "      <td>62.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.489051</td>\n",
       "      <td>3996.5</td>\n",
       "      <td>189.904397</td>\n",
       "      <td>188.535096</td>\n",
       "      <td>189.320332</td>\n",
       "      <td>105.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>0.813559</td>\n",
       "      <td>3851.5</td>\n",
       "      <td>194.783721</td>\n",
       "      <td>192.899912</td>\n",
       "      <td>194.122489</td>\n",
       "      <td>102.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>0.660000</td>\n",
       "      <td>4398.0</td>\n",
       "      <td>196.995954</td>\n",
       "      <td>194.350885</td>\n",
       "      <td>196.046576</td>\n",
       "      <td>90.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>0.575893</td>\n",
       "      <td>5605.5</td>\n",
       "      <td>194.249063</td>\n",
       "      <td>192.252870</td>\n",
       "      <td>193.937899</td>\n",
       "      <td>69.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>0.944444</td>\n",
       "      <td>5781.5</td>\n",
       "      <td>185.953703</td>\n",
       "      <td>184.640565</td>\n",
       "      <td>185.507932</td>\n",
       "      <td>52.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>0.734513</td>\n",
       "      <td>4162.0</td>\n",
       "      <td>196.237643</td>\n",
       "      <td>194.549705</td>\n",
       "      <td>195.608119</td>\n",
       "      <td>59.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Aspect ratio  Countour area   R average   G average   B average  \\\n",
       "9        0.409722         3500.0  188.999621  187.302455  188.562101   \n",
       "27       1.333333         3194.5  192.006318  190.058972  191.284239   \n",
       "30       0.726316         2955.5  193.060148  192.110591  192.664501   \n",
       "35       0.473214         6355.0  202.093351  199.387297  201.049964   \n",
       "42       0.489051         3996.5  189.904397  188.535096  189.320332   \n",
       "..            ...            ...         ...         ...         ...   \n",
       "836      0.813559         3851.5  194.783721  192.899912  194.122489   \n",
       "838      0.660000         4398.0  196.995954  194.350885  196.046576   \n",
       "841      0.575893         5605.5  194.249063  192.252870  193.937899   \n",
       "842      0.944444         5781.5  185.953703  184.640565  185.507932   \n",
       "843      0.734513         4162.0  196.237643  194.549705  195.608119   \n",
       "\n",
       "     R 0.25 % quantile  G 0.25 % quantile  B 0.25 % quantile  \\\n",
       "9                 72.0               48.0               63.0   \n",
       "27                90.0               49.0               73.0   \n",
       "30               110.0               71.0               97.0   \n",
       "35                62.0               37.0               49.0   \n",
       "42               105.0               61.0               90.0   \n",
       "..                 ...                ...                ...   \n",
       "836              102.0               60.0               87.0   \n",
       "838               90.0               49.0               74.0   \n",
       "841               69.0               40.0               55.0   \n",
       "842               52.0               31.0               41.0   \n",
       "843               59.0               35.0               46.0   \n",
       "\n",
       "     R 97.5 % quantile  G 97.5 % quantile  B 97.5 % quantile  Class  \n",
       "9                199.0              199.0              199.0    4.0  \n",
       "27               200.0              200.0              201.0    4.0  \n",
       "30               200.0              200.0              200.0    4.0  \n",
       "35               215.0              215.0              215.0    4.0  \n",
       "42               197.0              197.0              197.0    4.0  \n",
       "..                 ...                ...                ...    ...  \n",
       "836              202.0              202.0              202.0    4.0  \n",
       "838              206.0              206.0              206.0    4.0  \n",
       "841              205.0              205.0              206.0    4.0  \n",
       "842              197.0              197.0              197.0    4.0  \n",
       "843              208.0              208.0              208.0    4.0  \n",
       "\n",
       "[151 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_dnn[test_dataset_dnn['Class'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c532f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = scale_features(np.array(train_dataset_dnn.drop(columns = 'Class')))\n",
    "train_y = np.array(to_categorical(train_dataset_dnn['Class']))\n",
    "test_x = scale_features(np.array(test_dataset_dnn.drop(columns = 'Class')))\n",
    "test_y = np.array(to_categorical(test_dataset_dnn['Class']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73278c8",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28cf3b05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                120       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 55        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175\n",
      "Trainable params: 175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 06:37:28.962627: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-30 06:37:28.963043: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(10, activation=LeakyReLU(alpha=0.3), input_shape=(11,)))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a175ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('simple_dnn.h5')\n",
    "model.compile(optimizer=Adam(), \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a2343d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 1s 7ms/step - loss: 0.6393 - accuracy: 0.7533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6393169164657593, 0.7533300518989563]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8626361c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 8ms/step - loss: 1.3789 - accuracy: 0.5979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3789215087890625, 0.5978773832321167]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b7bf870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 06:37:44.769602: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "dnn_predictions = tf.math.argmax(model.predict(test_x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e0f58f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(dnn_predictions, test_dataset_dnn['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd082b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 54,   2,  14,   0,   0],\n",
       "       [ 77, 143,   0,   0,   0],\n",
       "       [ 11,   7,   2,   0,   0],\n",
       "       [ 59,  25, 124, 157,   0],\n",
       "       [  0,   0,   7,  15, 151]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe5e41bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_specimens = np.bincount(test_dataset_dnn['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b62d6c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([201, 177, 147, 172, 151])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_specimens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09fae98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26865672, 0.8079096 , 0.01360544, 0.9127907 , 1.        ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(cm) / total_specimens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f86951",
   "metadata": {},
   "source": [
    "# Trainning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a1d1a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [    \n",
    "    ModelCheckpoint('simple_dnn2.h5', verbose = 1, save_best_only = True, \n",
    "                    save_weights_only = True, monitor='val_acc')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f8fd57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 00:03:39.510470: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - ETA: 0s - loss: 0.6393 - acc: 0.7504 - precision_1: 0.8673 - recall_1: 0.5802"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 00:03:41.221253: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_acc improved from -inf to 0.59670, saving model to simple_dnn2.h5\n",
      "64/64 [==============================] - 3s 27ms/step - loss: 0.6393 - acc: 0.7504 - precision_1: 0.8673 - recall_1: 0.5802 - val_loss: 1.3911 - val_acc: 0.5967 - val_precision_1: 0.6478 - val_recall_1: 0.4988\n",
      "Epoch 2/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.6338 - acc: 0.7499 - precision_1: 0.8627 - recall_1: 0.6014\n",
      "Epoch 2: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.6338 - acc: 0.7499 - precision_1: 0.8627 - recall_1: 0.6014 - val_loss: 1.4165 - val_acc: 0.5731 - val_precision_1: 0.6356 - val_recall_1: 0.4752\n",
      "Epoch 3/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.6280 - acc: 0.7548 - precision_1: 0.8646 - recall_1: 0.5920\n",
      "Epoch 3: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.6280 - acc: 0.7548 - precision_1: 0.8646 - recall_1: 0.5920 - val_loss: 1.4420 - val_acc: 0.5696 - val_precision_1: 0.6139 - val_recall_1: 0.4800\n",
      "Epoch 4/60\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.6231 - acc: 0.7510 - precision_1: 0.8642 - recall_1: 0.5997\n",
      "Epoch 4: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 23ms/step - loss: 0.6231 - acc: 0.7509 - precision_1: 0.8649 - recall_1: 0.5999 - val_loss: 1.4488 - val_acc: 0.5719 - val_precision_1: 0.6124 - val_recall_1: 0.4788\n",
      "Epoch 5/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.6213 - acc: 0.7510 - precision_1: 0.8686 - recall_1: 0.5998\n",
      "Epoch 5: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 26ms/step - loss: 0.6184 - acc: 0.7538 - precision_1: 0.8710 - recall_1: 0.6029 - val_loss: 1.4212 - val_acc: 0.5861 - val_precision_1: 0.6246 - val_recall_1: 0.5024\n",
      "Epoch 6/60\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.6127 - acc: 0.7555 - precision_1: 0.8600 - recall_1: 0.6186\n",
      "Epoch 6: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.6137 - acc: 0.7548 - precision_1: 0.8587 - recall_1: 0.6177 - val_loss: 1.4636 - val_acc: 0.5613 - val_precision_1: 0.6093 - val_recall_1: 0.4965\n",
      "Epoch 7/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.6079 - acc: 0.7563 - precision_1: 0.8613 - recall_1: 0.6186\n",
      "Epoch 7: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.6079 - acc: 0.7563 - precision_1: 0.8613 - recall_1: 0.6186 - val_loss: 1.4190 - val_acc: 0.5814 - val_precision_1: 0.6431 - val_recall_1: 0.5035\n",
      "Epoch 8/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.6040 - acc: 0.7632 - precision_1: 0.8659 - recall_1: 0.6117\n",
      "Epoch 8: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.6040 - acc: 0.7632 - precision_1: 0.8659 - recall_1: 0.6117 - val_loss: 1.4411 - val_acc: 0.5778 - val_precision_1: 0.6287 - val_recall_1: 0.5212\n",
      "Epoch 9/60\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.5999 - acc: 0.7669 - precision_1: 0.8652 - recall_1: 0.6146\n",
      "Epoch 9: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.5999 - acc: 0.7667 - precision_1: 0.8642 - recall_1: 0.6152 - val_loss: 1.4762 - val_acc: 0.5719 - val_precision_1: 0.6129 - val_recall_1: 0.5153\n",
      "Epoch 10/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5960 - acc: 0.7632 - precision_1: 0.8594 - recall_1: 0.6270\n",
      "Epoch 10: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.5960 - acc: 0.7632 - precision_1: 0.8594 - recall_1: 0.6270 - val_loss: 1.4613 - val_acc: 0.5755 - val_precision_1: 0.6279 - val_recall_1: 0.5153\n",
      "Epoch 11/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.5910 - acc: 0.7601 - precision_1: 0.8637 - recall_1: 0.6290\n",
      "Epoch 11: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.5898 - acc: 0.7612 - precision_1: 0.8634 - recall_1: 0.6300 - val_loss: 1.4652 - val_acc: 0.5743 - val_precision_1: 0.6252 - val_recall_1: 0.5153\n",
      "Epoch 12/60\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.5888 - acc: 0.7649 - precision_1: 0.8589 - recall_1: 0.6310\n",
      "Epoch 12: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.5874 - acc: 0.7657 - precision_1: 0.8592 - recall_1: 0.6320 - val_loss: 1.4900 - val_acc: 0.5649 - val_precision_1: 0.6205 - val_recall_1: 0.5130\n",
      "Epoch 13/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5838 - acc: 0.7691 - precision_1: 0.8636 - recall_1: 0.6310\n",
      "Epoch 13: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.5838 - acc: 0.7691 - precision_1: 0.8636 - recall_1: 0.6310 - val_loss: 1.4805 - val_acc: 0.5708 - val_precision_1: 0.6225 - val_recall_1: 0.5212\n",
      "Epoch 14/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.5790 - acc: 0.7681 - precision_1: 0.8728 - recall_1: 0.6295\n",
      "Epoch 14: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.5781 - acc: 0.7686 - precision_1: 0.8747 - recall_1: 0.6300 - val_loss: 1.5268 - val_acc: 0.5590 - val_precision_1: 0.6033 - val_recall_1: 0.5130\n",
      "Epoch 15/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.5739 - acc: 0.7712 - precision_1: 0.8654 - recall_1: 0.6452\n",
      "Epoch 15: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.5731 - acc: 0.7711 - precision_1: 0.8662 - recall_1: 0.6453 - val_loss: 1.4924 - val_acc: 0.5743 - val_precision_1: 0.6130 - val_recall_1: 0.5212\n",
      "Epoch 16/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.5683 - acc: 0.7752 - precision_1: 0.8682 - recall_1: 0.6472\n",
      "Epoch 16: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 25ms/step - loss: 0.5698 - acc: 0.7736 - precision_1: 0.8667 - recall_1: 0.6448 - val_loss: 1.5045 - val_acc: 0.5625 - val_precision_1: 0.6138 - val_recall_1: 0.5153\n",
      "Epoch 17/60\n",
      "61/64 [===========================>..] - ETA: 0s - loss: 0.5674 - acc: 0.7782 - precision_1: 0.8671 - recall_1: 0.6419\n",
      "Epoch 17: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 25ms/step - loss: 0.5653 - acc: 0.7795 - precision_1: 0.8673 - recall_1: 0.6448 - val_loss: 1.5198 - val_acc: 0.5708 - val_precision_1: 0.6093 - val_recall_1: 0.5259\n",
      "Epoch 18/60\n",
      "61/64 [===========================>..] - ETA: 0s - loss: 0.5614 - acc: 0.7741 - precision_1: 0.8657 - recall_1: 0.6573\n",
      "Epoch 18: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.5616 - acc: 0.7741 - precision_1: 0.8668 - recall_1: 0.6581 - val_loss: 1.5287 - val_acc: 0.5637 - val_precision_1: 0.6061 - val_recall_1: 0.5189\n",
      "Epoch 19/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.5547 - acc: 0.7772 - precision_1: 0.8684 - recall_1: 0.6517\n",
      "Epoch 19: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.5568 - acc: 0.7775 - precision_1: 0.8674 - recall_1: 0.6517 - val_loss: 1.5390 - val_acc: 0.5590 - val_precision_1: 0.6106 - val_recall_1: 0.5142\n",
      "Epoch 20/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5518 - acc: 0.7884 - precision_1: 0.8678 - recall_1: 0.6542\n",
      "Epoch 20: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 23ms/step - loss: 0.5518 - acc: 0.7884 - precision_1: 0.8678 - recall_1: 0.6542 - val_loss: 1.5332 - val_acc: 0.5649 - val_precision_1: 0.6088 - val_recall_1: 0.5248\n",
      "Epoch 21/60\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.7758 - precision_1: 0.8663 - recall_1: 0.6652\n",
      "Epoch 21: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 30ms/step - loss: 0.5487 - acc: 0.7760 - precision_1: 0.8663 - recall_1: 0.6650 - val_loss: 1.5340 - val_acc: 0.5590 - val_precision_1: 0.6156 - val_recall_1: 0.5212\n",
      "Epoch 22/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.5455 - acc: 0.7908 - precision_1: 0.8749 - recall_1: 0.6557\n",
      "Epoch 22: val_acc did not improve from 0.59670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 1s 22ms/step - loss: 0.5451 - acc: 0.7908 - precision_1: 0.8757 - recall_1: 0.6566 - val_loss: 1.5450 - val_acc: 0.5637 - val_precision_1: 0.6091 - val_recall_1: 0.5236\n",
      "Epoch 23/60\n",
      "61/64 [===========================>..] - ETA: 0s - loss: 0.5408 - acc: 0.7879 - precision_1: 0.8719 - recall_1: 0.6660\n",
      "Epoch 23: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.5406 - acc: 0.7893 - precision_1: 0.8725 - recall_1: 0.6650 - val_loss: 1.5662 - val_acc: 0.5601 - val_precision_1: 0.6108 - val_recall_1: 0.5200\n",
      "Epoch 24/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.5377 - acc: 0.7954 - precision_1: 0.8777 - recall_1: 0.6653\n",
      "Epoch 24: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.5375 - acc: 0.7953 - precision_1: 0.8767 - recall_1: 0.6665 - val_loss: 1.5891 - val_acc: 0.5613 - val_precision_1: 0.6057 - val_recall_1: 0.5271\n",
      "Epoch 25/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5332 - acc: 0.7884 - precision_1: 0.8709 - recall_1: 0.6724\n",
      "Epoch 25: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.5332 - acc: 0.7884 - precision_1: 0.8709 - recall_1: 0.6724 - val_loss: 1.6454 - val_acc: 0.5519 - val_precision_1: 0.6122 - val_recall_1: 0.5342\n",
      "Epoch 26/60\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.5321 - acc: 0.7956 - precision_1: 0.8769 - recall_1: 0.6711\n",
      "Epoch 26: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 23ms/step - loss: 0.5301 - acc: 0.7967 - precision_1: 0.8777 - recall_1: 0.6729 - val_loss: 1.6408 - val_acc: 0.5578 - val_precision_1: 0.6059 - val_recall_1: 0.5295\n",
      "Epoch 27/60\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.5259 - acc: 0.7976 - precision_1: 0.8754 - recall_1: 0.6761\n",
      "Epoch 27: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.5262 - acc: 0.7972 - precision_1: 0.8748 - recall_1: 0.6759 - val_loss: 1.6362 - val_acc: 0.5590 - val_precision_1: 0.6054 - val_recall_1: 0.5318\n",
      "Epoch 28/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.5200 - acc: 0.7928 - precision_1: 0.8711 - recall_1: 0.6880\n",
      "Epoch 28: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 25ms/step - loss: 0.5219 - acc: 0.7928 - precision_1: 0.8716 - recall_1: 0.6867 - val_loss: 1.6165 - val_acc: 0.5578 - val_precision_1: 0.6120 - val_recall_1: 0.5283\n",
      "Epoch 29/60\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.8026 - precision_1: 0.8776 - recall_1: 0.6830\n",
      "Epoch 29: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 23ms/step - loss: 0.5186 - acc: 0.8027 - precision_1: 0.8776 - recall_1: 0.6828 - val_loss: 1.6372 - val_acc: 0.5613 - val_precision_1: 0.6148 - val_recall_1: 0.5401\n",
      "Epoch 30/60\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.5137 - acc: 0.8001 - precision_1: 0.8756 - recall_1: 0.6915\n",
      "Epoch 30: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 23ms/step - loss: 0.5146 - acc: 0.7992 - precision_1: 0.8755 - recall_1: 0.6907 - val_loss: 1.6268 - val_acc: 0.5637 - val_precision_1: 0.6180 - val_recall_1: 0.5436\n",
      "Epoch 31/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.5099 - acc: 0.8105 - precision_1: 0.8742 - recall_1: 0.6935\n",
      "Epoch 31: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.5118 - acc: 0.8091 - precision_1: 0.8729 - recall_1: 0.6912 - val_loss: 1.7081 - val_acc: 0.5566 - val_precision_1: 0.6142 - val_recall_1: 0.5389\n",
      "Epoch 32/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5074 - acc: 0.8106 - precision_1: 0.8905 - recall_1: 0.6941\n",
      "Epoch 32: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 29ms/step - loss: 0.5074 - acc: 0.8106 - precision_1: 0.8905 - recall_1: 0.6941 - val_loss: 1.6768 - val_acc: 0.5637 - val_precision_1: 0.6127 - val_recall_1: 0.5448\n",
      "Epoch 33/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5043 - acc: 0.8076 - precision_1: 0.8766 - recall_1: 0.6976\n",
      "Epoch 33: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.5043 - acc: 0.8076 - precision_1: 0.8766 - recall_1: 0.6976 - val_loss: 1.6996 - val_acc: 0.5554 - val_precision_1: 0.6107 - val_recall_1: 0.5366\n",
      "Epoch 34/60\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.5015 - acc: 0.8145 - precision_1: 0.8881 - recall_1: 0.6969\n",
      "Epoch 34: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 28ms/step - loss: 0.5028 - acc: 0.8135 - precision_1: 0.8874 - recall_1: 0.6961 - val_loss: 1.7087 - val_acc: 0.5613 - val_precision_1: 0.6120 - val_recall_1: 0.5413\n",
      "Epoch 35/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.4977 - acc: 0.8170 - precision_1: 0.8885 - recall_1: 0.7026\n",
      "Epoch 35: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4989 - acc: 0.8170 - precision_1: 0.8879 - recall_1: 0.7030 - val_loss: 1.6881 - val_acc: 0.5660 - val_precision_1: 0.6164 - val_recall_1: 0.5495\n",
      "Epoch 36/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.4950 - acc: 0.8090 - precision_1: 0.8828 - recall_1: 0.7021\n",
      "Epoch 36: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.4955 - acc: 0.8096 - precision_1: 0.8834 - recall_1: 0.7030 - val_loss: 1.7567 - val_acc: 0.5649 - val_precision_1: 0.6129 - val_recall_1: 0.5472\n",
      "Epoch 37/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4943 - acc: 0.8130 - precision_1: 0.8779 - recall_1: 0.7025\n",
      "Epoch 37: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.4943 - acc: 0.8130 - precision_1: 0.8779 - recall_1: 0.7025 - val_loss: 1.7648 - val_acc: 0.5578 - val_precision_1: 0.6131 - val_recall_1: 0.5401\n",
      "Epoch 38/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4885 - acc: 0.8234 - precision_1: 0.8938 - recall_1: 0.7099\n",
      "Epoch 38: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 25ms/step - loss: 0.4885 - acc: 0.8234 - precision_1: 0.8938 - recall_1: 0.7099 - val_loss: 1.7314 - val_acc: 0.5672 - val_precision_1: 0.6126 - val_recall_1: 0.5483\n",
      "Epoch 39/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4856 - acc: 0.8160 - precision_1: 0.8804 - recall_1: 0.7153\n",
      "Epoch 39: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.4856 - acc: 0.8160 - precision_1: 0.8804 - recall_1: 0.7153 - val_loss: 1.7431 - val_acc: 0.5613 - val_precision_1: 0.6126 - val_recall_1: 0.5389\n",
      "Epoch 40/60\n",
      "61/64 [===========================>..] - ETA: 0s - loss: 0.4841 - acc: 0.8207 - precision_1: 0.8898 - recall_1: 0.7157\n",
      "Epoch 40: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 25ms/step - loss: 0.4818 - acc: 0.8219 - precision_1: 0.8903 - recall_1: 0.7168 - val_loss: 1.7777 - val_acc: 0.5637 - val_precision_1: 0.6138 - val_recall_1: 0.5436\n",
      "Epoch 41/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4792 - acc: 0.8308 - precision_1: 0.8909 - recall_1: 0.7213\n",
      "Epoch 41: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.4792 - acc: 0.8308 - precision_1: 0.8909 - recall_1: 0.7213 - val_loss: 1.8073 - val_acc: 0.5637 - val_precision_1: 0.6127 - val_recall_1: 0.5448\n",
      "Epoch 42/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4764 - acc: 0.8254 - precision_1: 0.8863 - recall_1: 0.7188\n",
      "Epoch 42: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.4764 - acc: 0.8254 - precision_1: 0.8863 - recall_1: 0.7188 - val_loss: 1.7979 - val_acc: 0.5625 - val_precision_1: 0.6070 - val_recall_1: 0.5354\n",
      "Epoch 43/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4742 - acc: 0.8308 - precision_1: 0.8933 - recall_1: 0.7272\n",
      "Epoch 43: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 27ms/step - loss: 0.4742 - acc: 0.8308 - precision_1: 0.8933 - recall_1: 0.7272 - val_loss: 1.7980 - val_acc: 0.5684 - val_precision_1: 0.6135 - val_recall_1: 0.5483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/60\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.4707 - acc: 0.8333 - precision_1: 0.8994 - recall_1: 0.7272\n",
      "Epoch 44: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.4703 - acc: 0.8333 - precision_1: 0.8999 - recall_1: 0.7272 - val_loss: 1.8460 - val_acc: 0.5672 - val_precision_1: 0.6092 - val_recall_1: 0.5495\n",
      "Epoch 45/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.4710 - acc: 0.8231 - precision_1: 0.8855 - recall_1: 0.7288\n",
      "Epoch 45: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 23ms/step - loss: 0.4675 - acc: 0.8259 - precision_1: 0.8870 - recall_1: 0.7316 - val_loss: 1.8239 - val_acc: 0.5637 - val_precision_1: 0.6085 - val_recall_1: 0.5425\n",
      "Epoch 46/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4647 - acc: 0.8392 - precision_1: 0.9018 - recall_1: 0.7385\n",
      "Epoch 46: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.4647 - acc: 0.8392 - precision_1: 0.9018 - recall_1: 0.7385 - val_loss: 1.8228 - val_acc: 0.5660 - val_precision_1: 0.6126 - val_recall_1: 0.5483\n",
      "Epoch 47/60\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.4624 - acc: 0.8393 - precision_1: 0.9013 - recall_1: 0.7336\n",
      "Epoch 47: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 23ms/step - loss: 0.4621 - acc: 0.8397 - precision_1: 0.9013 - recall_1: 0.7341 - val_loss: 1.8249 - val_acc: 0.5719 - val_precision_1: 0.6170 - val_recall_1: 0.5566\n",
      "Epoch 48/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.4637 - acc: 0.8357 - precision_1: 0.8999 - recall_1: 0.7434\n",
      "Epoch 48: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.4606 - acc: 0.8387 - precision_1: 0.9021 - recall_1: 0.7459 - val_loss: 1.8426 - val_acc: 0.5708 - val_precision_1: 0.6126 - val_recall_1: 0.5519\n",
      "Epoch 49/60\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.4564 - acc: 0.8383 - precision_1: 0.8988 - recall_1: 0.7445\n",
      "Epoch 49: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.4578 - acc: 0.8372 - precision_1: 0.8986 - recall_1: 0.7430 - val_loss: 1.8984 - val_acc: 0.5672 - val_precision_1: 0.6073 - val_recall_1: 0.5472\n",
      "Epoch 50/60\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.4547 - acc: 0.8403 - precision_1: 0.8974 - recall_1: 0.7460\n",
      "Epoch 50: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.4553 - acc: 0.8397 - precision_1: 0.8962 - recall_1: 0.7454 - val_loss: 1.8919 - val_acc: 0.5684 - val_precision_1: 0.6115 - val_recall_1: 0.5531\n",
      "Epoch 51/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.4477 - acc: 0.8503 - precision_1: 0.9091 - recall_1: 0.7560\n",
      "Epoch 51: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 23ms/step - loss: 0.4516 - acc: 0.8461 - precision_1: 0.9068 - recall_1: 0.7533 - val_loss: 1.9232 - val_acc: 0.5696 - val_precision_1: 0.6092 - val_recall_1: 0.5495\n",
      "Epoch 52/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4492 - acc: 0.8485 - precision_1: 0.9082 - recall_1: 0.7568\n",
      "Epoch 52: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 25ms/step - loss: 0.4492 - acc: 0.8485 - precision_1: 0.9082 - recall_1: 0.7568 - val_loss: 1.9172 - val_acc: 0.5672 - val_precision_1: 0.6081 - val_recall_1: 0.5507\n",
      "Epoch 53/60\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.4478 - acc: 0.8457 - precision_1: 0.9058 - recall_1: 0.7584\n",
      "Epoch 53: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.4469 - acc: 0.8461 - precision_1: 0.9064 - recall_1: 0.7593 - val_loss: 1.9226 - val_acc: 0.5684 - val_precision_1: 0.6117 - val_recall_1: 0.5554\n",
      "Epoch 54/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.4410 - acc: 0.8528 - precision_1: 0.9055 - recall_1: 0.7631\n",
      "Epoch 54: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.4432 - acc: 0.8535 - precision_1: 0.9056 - recall_1: 0.7622 - val_loss: 1.8970 - val_acc: 0.5696 - val_precision_1: 0.6089 - val_recall_1: 0.5507\n",
      "Epoch 55/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.4437 - acc: 0.8483 - precision_1: 0.9066 - recall_1: 0.7586\n",
      "Epoch 55: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 23ms/step - loss: 0.4411 - acc: 0.8495 - precision_1: 0.9076 - recall_1: 0.7607 - val_loss: 1.9786 - val_acc: 0.5708 - val_precision_1: 0.6126 - val_recall_1: 0.5519\n",
      "Epoch 56/60\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.4394 - acc: 0.8527 - precision_1: 0.9033 - recall_1: 0.7649\n",
      "Epoch 56: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.4387 - acc: 0.8530 - precision_1: 0.9039 - recall_1: 0.7657 - val_loss: 1.9365 - val_acc: 0.5708 - val_precision_1: 0.6107 - val_recall_1: 0.5495\n",
      "Epoch 57/60\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4352 - acc: 0.8629 - precision_1: 0.9133 - recall_1: 0.7745\n",
      "Epoch 57: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.4352 - acc: 0.8629 - precision_1: 0.9133 - recall_1: 0.7745 - val_loss: 1.9311 - val_acc: 0.5731 - val_precision_1: 0.6118 - val_recall_1: 0.5613\n",
      "Epoch 58/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.4356 - acc: 0.8493 - precision_1: 0.9045 - recall_1: 0.7681\n",
      "Epoch 58: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.4349 - acc: 0.8505 - precision_1: 0.9053 - recall_1: 0.7686 - val_loss: 2.0342 - val_acc: 0.5731 - val_precision_1: 0.6149 - val_recall_1: 0.5554\n",
      "Epoch 59/60\n",
      "62/64 [============================>.] - ETA: 0s - loss: 0.4315 - acc: 0.8614 - precision_1: 0.9084 - recall_1: 0.7752\n",
      "Epoch 59: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.4309 - acc: 0.8624 - precision_1: 0.9088 - recall_1: 0.7770 - val_loss: 1.9610 - val_acc: 0.5696 - val_precision_1: 0.6127 - val_recall_1: 0.5578\n",
      "Epoch 60/60\n",
      "61/64 [===========================>..] - ETA: 0s - loss: 0.4290 - acc: 0.8601 - precision_1: 0.9093 - recall_1: 0.7751\n",
      "Epoch 60: val_acc did not improve from 0.59670\n",
      "64/64 [==============================] - 1s 22ms/step - loss: 0.4308 - acc: 0.8599 - precision_1: 0.9081 - recall_1: 0.7755 - val_loss: 2.0148 - val_acc: 0.5719 - val_precision_1: 0.6112 - val_recall_1: 0.5542\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = Adam(), loss='categorical_crossentropy', \n",
    "              metrics=['acc', metrics.Precision(), metrics.Recall()])\n",
    "dnn_data = model.fit(train_x, train_y, \n",
    "                    batch_size=32, epochs=60, callbacks=callbacks,\n",
    "                    validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7afe98f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploratory_performance = pd.DataFrame(dnn_data.history)\n",
    "exploratory_performance.to_csv(\"../../output_data/dnn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e41ac067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_precision_1</th>\n",
       "      <th>val_recall_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.639349</td>\n",
       "      <td>0.750370</td>\n",
       "      <td>0.867257</td>\n",
       "      <td>0.580168</td>\n",
       "      <td>1.391065</td>\n",
       "      <td>0.596698</td>\n",
       "      <td>0.647779</td>\n",
       "      <td>0.498821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.633776</td>\n",
       "      <td>0.749877</td>\n",
       "      <td>0.862703</td>\n",
       "      <td>0.601381</td>\n",
       "      <td>1.416454</td>\n",
       "      <td>0.573113</td>\n",
       "      <td>0.635647</td>\n",
       "      <td>0.475236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.627954</td>\n",
       "      <td>0.754810</td>\n",
       "      <td>0.864553</td>\n",
       "      <td>0.592008</td>\n",
       "      <td>1.441962</td>\n",
       "      <td>0.569575</td>\n",
       "      <td>0.613876</td>\n",
       "      <td>0.479953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.623122</td>\n",
       "      <td>0.750863</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.599901</td>\n",
       "      <td>1.448792</td>\n",
       "      <td>0.571934</td>\n",
       "      <td>0.612368</td>\n",
       "      <td>0.478774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.618413</td>\n",
       "      <td>0.753823</td>\n",
       "      <td>0.870991</td>\n",
       "      <td>0.602861</td>\n",
       "      <td>1.421160</td>\n",
       "      <td>0.586085</td>\n",
       "      <td>0.624633</td>\n",
       "      <td>0.502358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss       acc  precision_1  recall_1  val_loss   val_acc  \\\n",
       "0  0.639349  0.750370     0.867257  0.580168  1.391065  0.596698   \n",
       "1  0.633776  0.749877     0.862703  0.601381  1.416454  0.573113   \n",
       "2  0.627954  0.754810     0.864553  0.592008  1.441962  0.569575   \n",
       "3  0.623122  0.750863     0.864865  0.599901  1.448792  0.571934   \n",
       "4  0.618413  0.753823     0.870991  0.602861  1.421160  0.586085   \n",
       "\n",
       "   val_precision_1  val_recall_1  \n",
       "0         0.647779      0.498821  \n",
       "1         0.635647      0.475236  \n",
       "2         0.613876      0.479953  \n",
       "3         0.612368      0.478774  \n",
       "4         0.624633      0.502358  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploratory_performance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d4c3dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "exploratory_performance[['acc', 'val_acc']].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
